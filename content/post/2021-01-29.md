---
title: "在AWS上使用EMR创建一个简单的数据湖（3）"
date: 2021-01-29T10:44:40-08:00
lastmod: 2021-01-29T10:44:40-08:00
draft: false
keywords: []
description: ""
tags: []
categories: []
author: ""

# You can also close(false) or open(true) something for this content.
# P.S. comment can only be closed
comment: false
toc: false
autoCollapseToc: false
postMetaInFooter: false
hiddenFromHomePage: false
# You can also define another contentCopyright. e.g. contentCopyright: "This is another copyright."
contentCopyright: false
reward: false
mathjax: false
mathjaxEnableSingleDollar: false
mathjaxEnableAutoNumber: false

# You unlisted posts you might want not want the header or footer to show
hideHeaderAndFooter: false

# You can enable or disable out-of-date content warning for individual post.
# Comment this out to use the global config.
#enableOutdatedInfoWarning: false

flowchartDiagrams:
  enable: false
  options: ""

sequenceDiagrams: 
  enable: false
  options: ""

---

好的，当我们辛辛苦苦创建了EMR集群之后，有两个问题我们需要解决。第一，我们现在的EMR还没有权限访问S3，也就是说只能自娱自乐，搞些WordCount之类的玩具任务。第二，由于传统集群的“优良传统”，我们每次提交新的任务，都要远程登陆到EMR集群的某台机器上，把我们需要提交的Jar包拷贝上去，再用类似`spark-submit`这样的命令行来运行。全程都需要人工参与，毫无人工智能的爽感啊。接下来我们一个一个解决问题。

## 让EMR能够访问S3

要让EMR能够访问S3，我们需要给EMR的机器一个拥有适当权限的IAM角色。那么我们就看看具体需要什么权限吧。另外，写入日志也需要适当的权限，这里我们就一起处理了。

```Terraform
data "aws_iam_policy_document" "emr_instance_assume_role" {
  statement {
    principals {
      type        = "Service"
      identifiers = ["ec2.amazonaws.com"]
    }
    actions = ["sts:AssumeRole"]
  }
}

resource "aws_iam_role" "emr_instance_role" {
  name               = "${var.team_name}-emr-instance-role-${var.environment}"
  assume_role_policy = data.aws_iam_policy_document.emr_instance_assume_role.json
  tags               = local.common.tags
  path               = "/${var.team_name}/"
}

data "aws_iam_policy_document" "data_access_policy" {
  statement {
    actions = [
      "s3:AbortMultipartUpload",
      "s3:CreateBucket",
      "s3:DeleteObject",
      "s3:GetBucketVersioning",
      "s3:GetObject",
      "s3:GetObjectTagging",
      "s3:GetObjectVersion",
      "s3:ListBucket",
      "s3:ListBucketMultipartUploads",
      "s3:ListBucketVersions",
      "s3:ListMultipartUploadParts",
      "s3:ListObjects",
      "s3:ListObjectsV2",
      "s3:PutBucketVersioning",
      "s3:PutObject",
      "s3:PutObjectTagging",
    ]
    resources = [
      aws_s3_bucket.data.arn,
      aws_s3_bucket.emr_log.arn,
      "${aws_s3_bucket.data.arn}/*",
      "${aws_s3_bucket.emr_log.arn}/*"
    ]
  }
}

resource "aws_iam_policy" "data_access_policy" {
  name        = "${var.team_name}-emr-instance-policy-${var.environment}"
  path        = "/${var.team_name}/"
  description = "s3 access policy for EMR"
  policy      = data.aws_iam_policy_document.data_access_policy.json
}

resource "aws_iam_role_policy_attachment" "emr_service_role_policy_attachment" {
  role       = aws_iam_role.emr_service_role.name
  policy_arn = aws.aws_iam_policy.data_access_policy.arn
}
```

一样，创建空白角色，创建策略，将策略绑定到角色，一气呵成。最后，我们将这个角色应用到EMR上去。

```Terraform

resource "aws_iam_instance_profile" "emr_instance_profile" {
  name = "${var.team_name}-emr-instance-profile-${var.environment}"
  role = aws.aws_iam_policy.data_access_policy.name
}

resource "aws_emr_cluster" "etl_cluster" {
  ...
  ec2_attributes {
    instance_profile = aws.aws_iam_instance_profile.emr_instance_profile.arn
    ...
  }
}
```

我其实不是很喜欢这种设计，一层套一层。但是这样还是对策略的复用有好处的。既然代价已经付出了，那么以后再创建新的策略的时候，记得要复用角色啊。无论如何，现在我们的EMR集群就可以访问S3了。在Spark任务中，我们可以用以下这样的代码来写入和读取S3数据。

```Scala
val dataset = spark.write.parquet("s3://<bucket-name>/path")
// use dataset as normal
```

有了IAM角色的帮助，我们不用在Spark的配置中担心S3的各种密钥，所有的访问权限管理都交给了IAM。

但是，我们这样做只是把S3作为一个非常简单的文件系统来用，这个在大部分时候是不够用的。当我们用惯了Hive大数据库，和各种方便的SQL查询之后，再回归文件访问，总会有一种一朝回到解放前的感觉。因此，我们需要AWS Glue的帮助，给我们的S3提供一个类似于Hive的接口。

我们首先创建一个空白的Glue目录，也可以叫做一个数据库。我们之后创建的所有表都会在这个数据库中。

```Terraform
resource "aws_glue_catalog_database" "database" {
  name = "${var.team_name}-glue-database-${var.environment}"
}
```

之后，我们需要给EMR以访问Glue的权限。由于这个权限也是访问数据不可或缺的一部分，我们直接将权限添加到之前创建的`data_access_policy`策略中。

```Terraform
data "aws_caller_identity" "current" {
}

data "aws_iam_policy_document" "data_access_policy" {
  ...
  statement {
    actions = [
      "glue:GetDatabase",
      "glue:GetDatabases",
      "glue:CreateDatabase",
      "glue:GetUserDefinedFunctions",
      "glue:CreateTable",
      "glue:UpdateTable",
      "glue:DeleteTable",
      "glue:GetTable",
      "glue:GetTables",
      "glue:GetConnection",
      "glue:GetTableVersions",
      "glue:CreatePartition",
      "glue:BatchCreatePartition",
      "glue:UpdatePartition",
      "glue:DeletePartition",
      "glue:BatchDeletePartition",
      "glue:GetPartition",
      "glue:GetPartitions",
      "glue:BatchGetPartition",
      "glue:StartCrawler",
      "glue:GetCrawlers",
    ]
    resources = [
      "arn:aws:glue:us-east-1:${data.aws_caller_identity.current.account_id}:catalog",
      "arn:aws:glue:us-east-1:${data.aws_caller_identity.current.account_id}:database/default",
      "arn:aws:glue:us-east-1:${data.aws_caller_identity.current.account_id}:database/global_temp",
      "arn:aws:glue:us-east-1:${data.aws_caller_identity.current.account_id}:database/${var.team_name}-*",
      "arn:aws:glue:us-east-1:${data.aws_caller_identity.current.account_id}:table/*"
    ]
  }
}

```

之后，我们要对EMR做一些配置，使得Glue可以作为Hive接口暴露出来。既然已经要添加配置了，一些常用的性能优化配置也顺便做了，希望不是过早优化。不管怎样，这种优化都是各种坊间流传的都市传说，万一有用呢，是吧？最重要的其实只是`spark-hive-site`这一小节。这里我们使用AWS提供的工具类，把Glue包装成了Hive的样子。

```Terraform
data "template_file" "emr_config" {
  template = <<EOF
[
  {
      "Classification":"spark",
      "Properties":{
        "maximizeResourceAllocation": "true"
      }
  },
  {
      "Classification":"spark-defaults",
      "Properties":{
        "spark.network.timeout": "800s",
        "spark.executor.heartbeatInterval": "60s",
        "spark.sql.catalogImplementation": "hive",
        "spark.sql.sources.partitionOverwriteMode": "dynamic",
        "spark.dynamicAllocation.enabled": "true",
        "spark.dynamicAllocation.initialExecutors": "20",
        "spark.dynamicAllocation.maxExecutors": "80",
        "spark.dynamicAllocation.executorAllocationRatio": "1",
        "spark.yarn.scheduler.reporterThread.maxFailures": "5",
        "spark.shuffle.compress": "true",
        "spark.shuffle.spill.compress": "true",
        "spark.default.parallelism": "4000"
      }
  },
  {
    "Classification": "mapred-site",
    "Properties": {
      "mapreduce.map.output.compress": "true"
    }
  },
  {
      "Classification":"spark-hive-site",
      "Properties":{
        "hive.metastore.client.factory.class":"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory",
        "hive.metastore.connect.retries": "15",
        "hive.blobstore.optimizations.enabled": "false",
        "hive.exec.dynamic.partition": "true",
        "hive.exec.dynamic.partition.mode": "nonstrict",
        "hive.mv.files.thread": "20"
      }
  },
  {
      "Classification":"yarn-site",
      "Properties":{
        "yarn.nodemanager.vmem-check-enabled":"false",
        "yarn.nodemanager.pmem-check-enabled":"false"
      }
  }
]
EOF
}

resource "aws_emr_cluster" "etl_cluster" {
  ...
  configurations         = data.template_file.emr_config.rendered
}
```

好了，我们的EMR现在就可以使用方便的Hive来做S3的读写了。下面是一个读取的例子：

```Scala
val dataFrame = spark.sql(
  s"""
    | select * from test_table where date='2021-01-29' and data_type=456
  """.stripMargin
)
// use dataFrame as normal
```

等一下，我们怎么确定这个是在读S3呢？那个`test_table`从哪里冒出来的？我们回过头来看看test_table的创建过程。我们当然可以用Glue手动创建表，并且连接到S3。这也是Glue Crawler最常用的做法。但是，如果我们的数据主要都是从Spark任务中生成的，或者ETL任务写入S3，那么一事不烦二主，我们可以直接从Spark任务中创建指向S3的表，并保存在Glue中。

```Scala
val tableName = "test_table"
spark.sql(
  s"""
    | CREATE TABLE IF NOT EXIST $tableName
    |   (id BIGINT, name VARCHAR(100), data_type INT)
    | PARTITIONED BY (date VARCHAR(15))
    | STORED AS ORC
    | LOCATION 's3://<bucket-name>/hive/$tableName'
  """
)
```

这样，一个数据库就创建好了，Glue的元数据也同步了。之后对于这张表的使用，Glue这一层几乎就是透明的了。

## 通过Step操作远程向EMR提交任务

对于一个成熟的系统，我们自然希望能够尽可能的减少人工操作，尤其是一些重复性的任务。最典型的重复性任务，就是向EMR集群提交任务了。如果你创建EMR集群只是为了一个任务，那么当我没说。不过，大多数情况下，一个公司或者一个组创建了一个EMR集群，自然是希望能够尽可能高效的使用它，而不是每次提交一个任务，都要等20分钟来创建EMR集群。

这里，我们主要要考虑的是，SSH登录到EMR再提交任务这种做法，一是麻烦，二是不安全。所以，我们将目光投向AWS引入的更抽象的概念：步骤(Step)。EMR的一个步骤就是提交上去的一个任务，可以是Spark任务，也可以是EMR支持的各种软件的任务。这里我们只关注Spark任务。

用代码来提交一个任务也很简单，首先我们的Jar包应该被保存在S3桶里，并且这个S3桶要能够被EMR集群访问到。我们之前创建的数据桶就是个好选择。刚好我们可以把所有关于数据湖的东西都放在一起。然后，我们就需要访问AWS EMR本身的HTTP API了。代码如下（Kotlin版本）：

```Kotlin
val emr = AmazonElasticMapReduceClientBuilder.standard()
                .withRegion(Regions.US_EAST_1)
                .build()
val clusterName = "ETL-emr-cluster"

fun getClusterId(): String {
  val listClusterRequest = ListClustersRequest().withClusterStates("RUNNING", "WAITING")
  val response = emr.listClusters(listClusterRequest)
  val cluster = response.clusters.first { it.name == emrClusterName }
  return cluster.id
}

fun runJob(jobName: String, jarPathOnS3: String, className: String, additionalArgs: List<String>): String {
  val args = arrayOf(
          "spark-submit",
          "--class", className
      ) + additionalArgs + arrayOf(jarPathOnS3)
  val step = HadoopJarStepConfig()
      .withJar("command-runner.jar")
      .withArgs(*args)
  val stepConfig = StepConfig(jobName, step)
      .withActionOnFailure(ActionOnFailure.CONTINUE)
  val result = emr.addJobFlowSteps(
                AddJobFlowStepsRequest(getClusterId())
                        .withSteps(stepConfig)
  )
  return result.stepIds.first()
}

fun waitForStepFinish(stepId: String) {
  val describeStepRequest = DescribeStepRequest()
                .withStepId(stepId)
                .withClusterId(getClusterId())
        emr.waiters().stepComplete().run(WaiterParameters(describeStepRequest))
}
```

这里，为了避免我们的EMR集群由于各种原因挂掉又重建，我们每次都通过固定的集群名称来获得API所需要的集群ID。提交步骤是异步的，我们会得到一个步骤ID，这里我们也展示了通过步骤ID来等待步骤完成的函数。

那么，要运行这些函数，我们需要哪些权限呢？其实理论上来说我们只要`elasticmapreduce:AddJobFlowSteps`权限就好了。但是作为一个完整的任务系统，我们不可避免地要添加上步骤监控，步骤管理，甚至集群管理之类的功能，所以作为和EMR集群交互的任务服务，我们最好能添加如下权限：

```Terraform
data "aws_iam_policy_document" "step_manager_policy" {
  statement {
    actions = [
      "elasticmapreduce:AddJobFlowSteps",
      "elasticmapreduce:CancelSteps",
      "elasticmapreduce:DescribeCluster",
      "elasticmapreduce:DescribeStep",
      "elasticmapreduce:ListSteps",
      "elasticmapreduce:ListSteps",
      "elasticmapreduce:ListInstanceGroups",
      "elasticmapreduce:ModifyInstanceGroups",
      "elasticmapreduce:ModifyCluster",
    ]
    resources = ["*"]
    condition {
      test = "StringEquals"
      variable = "elasticmapreduce:ResourceTag/useTag"
      values = ["etl"]
    }
  }
  statement {
    actions = [
      "elasticmapreduce:ListClusters",
    ]
    resources = ["*"]
  }
}
```
